{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b944ac92",
   "metadata": {},
   "source": [
    "# Cadenas de Markov\n",
    "\n",
    "## Profesor: Doctor Simón Lunagómez\n",
    "\n",
    "## Darío Sotelo\n",
    "## Mateo Guajardo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52148dcc",
   "metadata": {},
   "source": [
    "### Introducción\n",
    "\n",
    "Este proyeto se trata de un cierto tipo de proceso. La característica principal de este proceso es que no retiene información del pasado. Este proceso se llama $ \\textit{Proceso de Markov.} $ Dentro de este tipo de procesos, vamos a dirigir el análisis de este proyecto a los procesos de Markov en los que existe un número finito de estados. A este tipo de procesos se les conoce como $ \\textit{Cadenas de Markov.} $ Este concepto fue presentado por primera vez por Andrey Markov en 1906, y algo que hace este tema tan importante es la característica de pérdida de memoria, en la que el resultado de un proceso aleatorio a tiempo $ t $ > 0 no depende de su pasado. Las cadenas de Markov viven tanto en el espacio como en el tiempo. Analizaremos las Cadenas de Markov a tiempo discreto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e66d994",
   "metadata": {},
   "source": [
    "### Definiciones\n",
    "\n",
    "- $ \\textit{Espacio de estados:} $ se refiere a todos los posibles valores que puede tomar $ X_n $ donde $ n $ representa la evolución del proceso a través del tiempo.\n",
    "- $ \\textit{Cadena de Markov:} $ una secuencia de variables aleatorias $ X_0 $, $ X_1 $, $ X_2 $,... tomando valores en el $ \\textit{espacio de estados} $ $\\{1,2,3,4,...\\}$ se llama $ \\textit{cadena de Markov} $ si $\\forall$ $ n $ > 0, \n",
    "\n",
    "\n",
    "$$P(X_{n+1}=j|X_n=i,X_{n-1}=i-1,...,X_0=i_0)=P(X_{n+1}=j|X_n=i).$$\n",
    "\n",
    "    \n",
    "- $ \\textit{Probabilidad de transición:} $ la cantidad $P(X_{n+1}=j|X_n=i)$ se llama $\\textit{probabilidad de transición} $ del estado $ i $ al estado $ j. $\n",
    "- $ \\textit{Propiedad de Markov:} $ dada toda la historia de las variables aleatorias anteriores, $X_n,X_{n-1},...,X_0,$ solo el $\\textit{último término}$ $ X_n $ importa para predecir $ X_{n+1}. $\n",
    "- $ \\textit{Matriz de transición:}$ sea $X_0,X_1,X_2...$ una cadena de Markov con espacio de estados $\\{1,2,...,M\\}$ y sea $q_{ij}=P(X_{n+1}=j|X_n=i)$ la probabilidad de transición del estado $i$ al estado $j.$ Entonces, $Q=(q_{ij})$, la matriz de tamaño $M$$\\times$$M$ $Q=(q_{ij})$, se llama la $\\textit{matriz de transición}$ de la cadena. $Q$ es una matriz no negativa cuyos renglones suman $1$.\n",
    "- $\\textit{Diagramas:}$ son las representaciones gráficas de las matrices de transición de un estado a otro."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13d2809",
   "metadata": {},
   "source": [
    "### Matrices de transición y diagramas\n",
    "\n",
    "Como fue explicado anteriormente de manera breve, las matrices de transición son la representación matricial de una $\\textit{cadena de Markov,}$ una característica muy importante de estas matrices es que todos los renglones deben sumar $1.$ Si una $\\textit{matriz de transición}$ cumple que sus columnas también sumen $1,$ a esta matriz se le llama $\\textit{doblemente estocástica.}$ Además, decimos que una matriz $P=(p_{ij}:i,j\\in I)$ es $\\textit{estocástica}$ si cada renglón $P=(p_{ij}:j\\in I)$ es una distribución. Un par de ejemplos de una matriz de transición (J. R. Norris):\n",
    "\n",
    "![matriz de transción](matricesDeTransicion.PNG)\n",
    "\n",
    "En este ejemplo, lo que está a la izquierda son las matrices de transición y los grafos que están a la derecha son sus $\\textit{diagramas.}$ Estas son dos formas diferentes de representar la misma una información, una $\\textit{cadena de Markov.}$ Los $\\textit{diagramas}$ son muy útiles porque permiten ver los nodos como estados y las aristas representan la probabilidad que se tiene de pasar de un estado a otro. En el ejemplo que se mostró anteriormente, podemos ver que la probabilidad de que si estamos en el estado $1$ y queremos ir al estado $2$ $p_{12}$ es $\\alpha,$ tenemos también la probabilidad de que la transición sea del estado $1$ al mismo $p_{11}$ es de $1-\\alpha$ en este ejemplo no vemos la arista que enseña esta probabilidad de transición, pero asumimos que se encuentra ahí. En el segundo ejemplo, la probabilidad de transición de $p_{11}=p_{13}=0$ esto quiere decir que no hay aristas que van en esas direcciones.\n",
    "\n",
    "Podemos definir a las $\\textit{cadenas de Markov}$ más formalmente de la siguiente manera: decimos que \n",
    "\n",
    "1. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce600c9",
   "metadata": {},
   "source": [
    "### Referencias\n",
    "\n",
    ">Norris, J. R. (1997). *Markov Chains* (2nd ed.). Cambridge University Press.\n",
    "\n",
    ">Blitzstein, J. K., & Hwang, J. (2015). *Introduction to probability* (2nd ed.). CRC Press.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85209281",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
